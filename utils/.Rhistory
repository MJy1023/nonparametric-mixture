plot(XX[Tree$DataPoints[1][[1]],], pch = 16, cex = 0.5, main = "Data")
points(XX[Tree$DataPoints[2][[1]],], col="red")
points(XX[Tree$DataPoints[3][[1]],], col="green")
points(XX[Tree$DataPoints[4][[1]],], col="blue")
points(XX[Tree$DataPoints[5][[1]],], col=brewer.pal(n = 9, name = "YlGnBu")[5])
points(XX[Tree$DataPoints[5][[1]],], col=brewer.pal(n = 9, name = "YlGnBu")[5])
##### Generating Data
library(freqparcoord)
library("TDA")
X1 <- cbind(rnorm(300, 1, .8), rnorm(300, 5, 0.8))
X2 <- cbind(rnorm(300, 3.5, .8), rnorm(300, 5, 0.8))
X3 <- cbind(rnorm(300, 6, 1), rnorm(300, 1, 1))
XX <- rbind(X1, X2, X3)
Tree <- clusterTree(XX, k = 100, density = "knn",printProgress = FALSE)
TreeKDE <- clusterTree(XX, k = 100, h = 0.3, density = "kde",printProgress = FALSE)
plot(Tree, type = "lambda", main = "lambda Tree (knn)")
plot(Tree, type = "kappa", main = "kappa Tree (knn)")
plot(TreeKDE, type = "lambda", main = "lambda Tree (kde)")
plot(TreeKDE, type = "kappa", main = "kappa Tree (kde)")
plot(XX[Tree$DataPoints[1][[1]],], pch = 16, cex = 0.5, main = "Data")
points(XX[Tree$DataPoints[2][[1]],], col="red")
points(XX[Tree$DataPoints[3][[1]],], col="green")
points(XX[Tree$DataPoints[4][[1]],], col="blue")
points(XX[Tree$DataPoints[5][[1]],], col=brewer.pal(n = 9, name = "YlGnBu")[5])
plot(XX[Tree$DataPoints[1][[1]],], pch = 16, cex = 0.5, main = "Data")
points(XX[Tree$DataPoints[2][[1]],], col="red")
points(XX[Tree$DataPoints[3][[1]],], col="green")
points(XX[Tree$DataPoints[4][[1]],], col="blue")
points(XX[Tree$DataPoints[5][[1]],], col="yellow")
View(Tree)
View(Tree)
library("TDA")# 四个子类
library(MASS)
# 每个类100个点
mean1 <- matrix(c(0,5),nrow = 2)
Sigma <- matrix(c(1,0,0,1),2,2)
D = mvrnorm(n = 100, mean1, Sigma)
D = cbind(matrix(1, nr = 100, nc = 1), D)
mean2 <- matrix(c(5,0),nrow = 2)
D = rbind(D, cbind(matrix(2, nr = 100, nc = 1), mvrnorm(n = 100, mean2, Sigma)))
mean3 <- matrix(c(0,-5),nrow = 2)
D = rbind(D, cbind(matrix(3, nr = 100, nc = 1), mvrnorm(n = 100, mean3, Sigma)))
mean4 <- matrix(c(-5,0),nrow = 2)
D = rbind(D, cbind(matrix(4, nr = 100, nc = 1), mvrnorm(n = 100, mean4, Sigma)))
X0= D[,2:3]
X = scale(X0)
Tree <- clusterTree(X, k = 100, density = "knn",printProgress = FALSE)
TreeKDE <- clusterTree(X, k = 100, h = 0.3, density = "kde",printProgress = FALSE)
plot(Tree, type = "lambda", main = "lambda Tree (knn)")
plot(Tree, type = "kappa", main = "kappa Tree (knn)")
plot(TreeKDE, type = "lambda", main = "lambda Tree (kde)")
plot(TreeKDE, type = "kappa", main = "kappa Tree (kde)")
plot(XX[Tree$DataPoints[1][[1]],], pch = 16, cex = 0.5, main = "Data")
points(XX[Tree$DataPoints[2][[1]],], col="red")
points(XX[Tree$DataPoints[3][[1]],], col="green")
points(XX[Tree$DataPoints[4][[1]],], col="blue")
points(XX[Tree$DataPoints[5][[1]],], col="yellow")
library("TDA")# 四个子类
library(MASS)
# 每个类100个点
mean1 <- matrix(c(0,5),nrow = 2)
Sigma <- matrix(c(1,0,0,1),2,2)
D = mvrnorm(n = 100, mean1, Sigma)
D = cbind(matrix(1, nr = 100, nc = 1), D)
mean2 <- matrix(c(5,0),nrow = 2)
D = rbind(D, cbind(matrix(2, nr = 100, nc = 1), mvrnorm(n = 100, mean2, Sigma)))
mean3 <- matrix(c(0,-5),nrow = 2)
D = rbind(D, cbind(matrix(3, nr = 100, nc = 1), mvrnorm(n = 100, mean3, Sigma)))
mean4 <- matrix(c(-5,0),nrow = 2)
D = rbind(D, cbind(matrix(4, nr = 100, nc = 1), mvrnorm(n = 100, mean4, Sigma)))
X0= D[,2:3]
X = scale(X0)
Tree <- clusterTree(X, k = 100, density = "knn",printProgress = FALSE)
TreeKDE <- clusterTree(X, k = 100, h = 0.3, density = "kde",printProgress = FALSE)
plot(Tree, type = "lambda", main = "lambda Tree (knn)")
plot(Tree, type = "kappa", main = "kappa Tree (knn)")
plot(TreeKDE, type = "lambda", main = "lambda Tree (kde)")
plot(TreeKDE, type = "kappa", main = "kappa Tree (kde)")
plot(X[Tree$DataPoints[1][[1]],], pch = 16, cex = 0.5, main = "Data")
points(X[Tree$DataPoints[2][[1]],], col="red")
points(X[Tree$DataPoints[3][[1]],], col="green")
points(X[Tree$DataPoints[4][[1]],], col="blue")
points(X[Tree$DataPoints[5][[1]],], col="yellow")
library("TDA")# 四个子类
library(MASS)
# 每个类100个点
D = rnorm(n = 100, 5, 1)
D = cbind(matrix(1, nr = 100, nc = 1), D)
D = rbind(D, cbind(matrix(2, nr = 100, nc = 1), rnorm(n = 100, 0, 1)))
D = rbind(D, cbind(matrix(3, nr = 100, nc = 1), mvrnorm(n = 100, -5, 1)))
X0= D[,2:2]
X = scale(X0)
Tree <- clusterTree(X, k = 100, density = "knn",printProgress = FALSE)
TreeKDE <- clusterTree(X, k = 100, h = 0.3, density = "kde",printProgress = FALSE)
plot(Tree, type = "lambda", main = "lambda Tree (knn)")
plot(Tree, type = "kappa", main = "kappa Tree (knn)")
plot(TreeKDE, type = "lambda", main = "lambda Tree (kde)")
plot(TreeKDE, type = "kappa", main = "kappa Tree (kde)")
library("TDA")# 四个子类
library(MASS)
# 每个类100个点
D = rnorm(n = 100, 5, 1)
D = cbind(matrix(1, nr = 100, nc = 1), D)
D = rbind(D, cbind(matrix(2, nr = 100, nc = 1), rnorm(n = 100, 0, 1)))
D = rbind(D, cbind(matrix(3, nr = 100, nc = 1), mvrnorm(n = 100, -5, 1)))
X0= D[,2:2]
X = scale(X0)
Tree <- clusterTree(X, k = 100, density = "knn",printProgress = FALSE)
TreeKDE <- clusterTree(X, k = 100, h = 0.3, density = "kde",printProgress = FALSE)
plot(Tree, type = "lambda", main = "lambda Tree (knn)")
plot(Tree, type = "kappa", main = "kappa Tree (knn)")
plot(TreeKDE, type = "lambda", main = "lambda Tree (kde)")
plot(TreeKDE, type = "kappa", main = "kappa Tree (kde)")
View(Tree)
View(TreeKDE)
source("D:/科研/Code/nonparametric mixture/utils/EMC_mod.R")
library("TDA")# 四个子类
library(MASS)
source("EMC_mod.R")
# 每个类100个点
D = rnorm(n = 100, 6, 0.25)
D = cbind(matrix(1, nr = 100, nc = 1), D)
D = rbind(D, cbind(matrix(2, nr = 100, nc = 1), rnorm(n = 100, 2, 0.25)))
D = rbind(D, cbind(matrix(3, nr = 100, nc = 1), mvrnorm(n = 100, -2, 0.25)))
D = rbind(D, cbind(matrix(4, nr = 100, nc = 1), mvrnorm(n = 100, -6, 0.25)))
X0 = D[,2:2]
X = scale(X0)
yy = density(X)
plot(yy, type="l", ylab="Density", xlab="x", las=1, lwd=2)
points(X, y=rep(0,400), pch=1)
## EMC
X_emc = EMC(X)
X_emc = EMC(X)
nonpara_result = X_emc$labels
## Visualization
plot(yy, type="l", ylab="Density", xlab="x", las=1, lwd=2)
points(X, y=rep(0,400), pch=1, col=rainbow(max(nonpara_result))[nonpara_result])
## using library('TDA')
Tree <- clusterTree(X, k = 100, density = "knn",printProgress = FALSE)
TreeKDE <- clusterTree(X, k = 100, h = 0.3, density = "kde",printProgress = FALSE)
plot(Tree, type = "lambda", main = "lambda Tree (knn)")
plot(Tree, type = "kappa", main = "kappa Tree (knn)")
plot(TreeKDE, type = "lambda", main = "lambda Tree (kde)")
plot(TreeKDE, type = "kappa", main = "kappa Tree (kde)")
nonpara_result <- vector(length = 400)
for (i in 1:length(TreeKDE$DataPoints)) {
for (j in TreeKDE$DataPoints[i][[1]]) {
nonpara_result[j] = i
}
}
## Visualization
plot(yy, type="l", ylab="Density", xlab="x", las=1, lwd=2)
points(X, y=rep(0,400), pch=1, col=rainbow(max(nonpara_result))[nonpara_result])
D = rnorm(n = 100, 6, 0.25)
D = cbind(matrix(1, nr = 100, nc = 1), D)
D = rbind(D, cbind(matrix(2, nr = 100, nc = 1), rnorm(n = 100, 2, 0.25)))
X0 = D[,2:2]
X = scale(X0)
yy = density(X)
plot(yy, type="l", ylab="Density", xlab="x", las=1, lwd=2)
points(X, y=rep(0,400), pch=1)
points(X, y=rep(0,200), pch=1)
## (1.1) Nonarametric clustering: Enhanced mode clustering
## 前提条件: 至少三个类
X_emc = EMC(X)
nonpara_result = X_emc$labels
## Visualization
plot(yy, type="l", ylab="Density", xlab="x", las=1, lwd=2)
points(X, y=rep(0,400), pch=1, col=rainbow(max(nonpara_result))[nonpara_result])
## Visualization
plot(yy, type="l", ylab="Density", xlab="x", las=1, lwd=2)
points(X, y=rep(0,200), pch=1, col=rainbow(max(nonpara_result))[nonpara_result])
ncol(X)
nrow(X)
library("TDA")# 四个子类
library(MASS)
source("EMC_mod.R")
# 每个类100个点
D = rnorm(n = 100, 6, 0.25)
D = cbind(matrix(1, nr = 100, nc = 1), D)
D = rbind(D, cbind(matrix(2, nr = 100, nc = 1), rnorm(n = 100, 2, 0.25)))
# D = rbind(D, cbind(matrix(3, nr = 100, nc = 1), mvrnorm(n = 100, -2, 0.25)))
# D = rbind(D, cbind(matrix(4, nr = 100, nc = 1), mvrnorm(n = 100, -6, 0.25)))
X0 = D[,2:2]
X = scale(X0)
yy = density(X)
plot(yy, type="l", ylab="Density", xlab="x", las=1, lwd=2)
points(X, y=rep(0,nrow(X)), pch=1)
## (1.1) Nonarametric clustering: Enhanced mode clustering
X_emc = EMC(X)
nonpara_result = X_emc$labels
## Visualization
plot(yy, type="l", ylab="Density", xlab="x", las=1, lwd=2)
points(X, y=rep(0,nrow(X)), pch=1, col=rainbow(max(nonpara_result))[nonpara_result])
## (1.2) Nonarametric clustering: "Density Clustering" in library('TDA')
Tree <- clusterTree(X, k = 100, density = "knn",printProgress = FALSE)
TreeKDE <- clusterTree(X, k = 100, h = 0.3, density = "kde",printProgress = FALSE)
plot(Tree, type = "lambda", main = "lambda Tree (knn)")
plot(Tree, type = "kappa", main = "kappa Tree (knn)")
plot(TreeKDE, type = "lambda", main = "lambda Tree (kde)")
plot(TreeKDE, type = "kappa", main = "kappa Tree (kde)")
nonpara_result <- vector(length = 400)
for (i in 1:length(TreeKDE$DataPoints)) {
for (j in TreeKDE$DataPoints[i][[1]]) {
nonpara_result[j] = i
}
}
## Visualization
plot(yy, type="l", ylab="Density", xlab="x", las=1, lwd=2)
points(X, y=rep(0,nrow(X)), pch=1, col=rainbow(max(nonpara_result))[nonpara_result])
# 每个类100个点
D = rnorm(n = 100, 6, 0.25)
D = cbind(matrix(1, nr = 100, nc = 1), D)
D = rbind(D, cbind(matrix(2, nr = 100, nc = 1), rnorm(n = 100, 2, 0.25)))
D = rbind(D, cbind(matrix(3, nr = 100, nc = 1), mvrnorm(n = 100, -2, 0.25)))
D = rbind(D, cbind(matrix(4, nr = 100, nc = 1), mvrnorm(n = 100, -6, 0.25)))
X0 = D[,2:2]
X = scale(X0)
yy = density(X)
plot(yy, type="l", ylab="Density", xlab="x", las=1, lwd=2)
points(X, y=rep(0,nrow(X)), pch=1)
## (1.1) Nonarametric clustering: Enhanced mode clustering
X_emc = EMC(X)
nonpara_result = X_emc$labels
## Visualization
plot(yy, type="l", ylab="Density", xlab="x", las=1, lwd=2)
points(X, y=rep(0,nrow(X)), pch=1, col=rainbow(max(nonpara_result))[nonpara_result])
##### Generating Data
library(freqparcoord)
library('TDA')
# 四个子类
# 每个类100个点
mean1 <- matrix(c(0,5),nrow = 2)
Sigma <- matrix(c(1,0,0,1),2,2)
D = mvrnorm(n = 100, mean1, Sigma)
D = cbind(matrix(1, nr = 100, nc = 1), D)
mean2 <- matrix(c(5,0),nrow = 2)
D = rbind(D, cbind(matrix(2, nr = 100, nc = 1), mvrnorm(n = 100, mean2, Sigma)))
# mean3 <- matrix(c(0,-5),nrow = 2)
# D = rbind(D, cbind(matrix(3, nr = 100, nc = 1), mvrnorm(n = 100, mean3, Sigma)))
#
# mean4 <- matrix(c(-5,0),nrow = 2)
# D = rbind(D, cbind(matrix(4, nr = 100, nc = 1), mvrnorm(n = 100, mean4, Sigma)))
# 可加outliers
head(D)
X0= D[,2:3]
X = scale(X0)
plot(X, xlim = c(-4,4),ylim = c(-4,4), main="Scaled original data")
Y = as.numeric(D[,1])
## (1.1) Nonarametric clustering: Enhanced mode clustering
## 前提条件: 大于等于2维，至少三个类
X_emc = EMC(X)
nonpara_result = X_emc$labels
## Visualization
plot(X, xlim = c(-4,4),ylim = c(-4,4), main="Results of Nonparametric Cluster", col=rainbow(max(nonpara_result))[nonpara_result])
source("D:/科研/Code/nonparametric mixture/utils/EMC_mod.R")
library("TDA")# 四个子类
library(MASS)
source("EMC_mod.R")
# 每个类100个点
D = rnorm(n = 100, 6, 0.25)
D = cbind(matrix(1, nr = 100, nc = 1), D)
D = rbind(D, cbind(matrix(2, nr = 100, nc = 1), rnorm(n = 100, 2, 0.25)))
X0 = D[,2:2]
X = scale(X0)
yy = density(X)
plot(yy, type="l", ylab="Density", xlab="x", las=1, lwd=2)
points(X, y=rep(0,nrow(X)), pch=1)
X_emc = EMC(X)
nonpara_result = X_emc$labels
## Visualization
plot(yy, type="l", ylab="Density", xlab="x", las=1, lwd=2)
points(X, y=rep(0,nrow(X)), pch=1, col=rainbow(max(nonpara_result))[nonpara_result])
library("TDA")# 四个子类
library(MASS)
source("EMC_mod.R")
# 每个类100个点
D = rnorm(n = 100, 6, 0.25)
D = cbind(matrix(1, nr = 100, nc = 1), D)
D = rbind(D, cbind(matrix(2, nr = 100, nc = 1), rnorm(n = 100, 2, 0.25)))
D = rbind(D, cbind(matrix(3, nr = 100, nc = 1), mvrnorm(n = 100, -2, 0.25)))
D = rbind(D, cbind(matrix(4, nr = 100, nc = 1), mvrnorm(n = 100, -6, 0.25)))
X0 = D[,2:2]
X = scale(X0)
yy = density(X)
plot(yy, type="l", ylab="Density", xlab="x", las=1, lwd=2)
points(X, y=rep(0,nrow(X)), pch=1)
## (1.1) Nonarametric clustering: Enhanced mode clustering
X_emc = EMC(X)
cluster_lab = X_emc$labels
source("D:/科研/Code/nonparametric mixture/utils/EMC_mod.R")
library("TDA")# 四个子类
library(MASS)
source("EMC_mod.R")
# 每个类100个点
D = rnorm(n = 100, 6, 0.25)
D = cbind(matrix(1, nr = 100, nc = 1), D)
D = rbind(D, cbind(matrix(2, nr = 100, nc = 1), rnorm(n = 100, 2, 0.25)))
D = rbind(D, cbind(matrix(3, nr = 100, nc = 1), mvrnorm(n = 100, -2, 0.25)))
D = rbind(D, cbind(matrix(4, nr = 100, nc = 1), mvrnorm(n = 100, -6, 0.25)))
X0 = D[,2:2]
X = scale(X0)
yy = density(X)
plot(yy, type="l", ylab="Density", xlab="x", las=1, lwd=2)
points(X, y=rep(0,nrow(X)), pch=1)
## (1.1) Nonarametric clustering: Enhanced mode clustering
X_emc = EMC(X)
cluster_lab = X_emc$labels
## Visualization
plot(yy, type="l", ylab="Density", xlab="x", las=1, lwd=2)
points(X, y=rep(0,nrow(X)), pch=1, col=rainbow(max(cluster_lab))[cluster_lab])
single_X = X[which(cluster_lab==1)]
single_X
nrow(single_X)
ncol(single_X)
length(single_X)
length(sing_X) / nrow(X)
length(single_X) / nrow(X)
p1 <- hist(x,breaks=50, include.lowest=FALSE, right=FALSE)
num_of_samples = 1000
x <- rgamma(num_of_samples, shape = 10, scale = 3)
x <- x + rnorm(length(x), mean=0, sd = .1)
p1 <- hist(x,breaks=50, include.lowest=FALSE, right=FALSE)
a <- chisq.test(p1$counts, p=null.probs, rescale.p=TRUE, simulate.p.value=TRUE)
library('zoo')
num_of_samples = 1000
x <- rgamma(num_of_samples, shape = 10, scale = 3)
x <- x + rnorm(length(x), mean=0, sd = .1)
p1 <- hist(x,breaks=50, include.lowest=FALSE, right=FALSE)
View(p1)
p1$breaks
breaks_cdf <- pgamma(p1$breaks, shape=10, scale=3)
breaks_cdf
null.probs <- rollapply(breaks_cdf, 2, function(x) x[2]-x[1])
library('zoo')
install.packages("zoo")
library('zoo')
null.probs <- rollapply(breaks_cdf, 2, function(x) x[2]-x[1])
null.probs
a <- chisq.test(p1$counts, p=null.probs, rescale.p=TRUE, simulate.p.value=TRUE)
View(a)
breaks_cdf <- pgamma(p1$breaks, shape=8, scale=3) ## Reference distribution
null.probs <- rollapply(breaks_cdf, 2, function(x) x[2]-x[1])
a <- chisq.test(p1$counts, p=null.probs, rescale.p=TRUE, simulate.p.value=TRUE)
View(a)
View(a)
breaks_cdf <- pgamma(p1$breaks, shape=11, scale=3) ## Reference distribution
null.probs <- rollapply(breaks_cdf, 2, function(x) x[2]-x[1])
a <- chisq.test(p1$counts, p=null.probs, rescale.p=TRUE, simulate.p.value=TRUE)
View(a)
View(a)
breaks_cdf <- pgamma(p1$breaks, shape=9, scale=3) ## Reference distribution
null.probs <- rollapply(breaks_cdf, 2, function(x) x[2]-x[1])
a <- chisq.test(p1$counts, p=null.probs, rescale.p=TRUE, simulate.p.value=TRUE)
View(a)
breaks_cdf <- pgamma(p1$breaks, shape=10, scale=3) ## Reference distribution
null.probs <- rollapply(breaks_cdf, 2, function(x) x[2]-x[1])
a <- chisq.test(p1$counts, p=null.probs, rescale.p=TRUE, simulate.p.value=TRUE)
View(a)
View(a)
## Cramér–von Mises criterion
y <- rgamma(num_of_samples, shape = 10, scale = 3)
res <- CramerVonMisesTwoSamples(x,y)
p-value = 1/6*exp(-res)
## p-value > significant level, 0.01 <= significant level <= 0.1
## Chi Square test
library('zoo')
res <- CramerVonMisesTwoSamples(x,y)
install.packages("CDFt")
library('CDFt')
y <- rgamma(num_of_samples, shape = 10, scale = 3)
res <- CramerVonMisesTwoSamples(x,y)
p-value = 1/6*exp(-res)
## Cramér–von Mises criterion
library('CDFt')
y <- rgamma(num_of_samples, shape = 10, scale = 3)
res <- CramerVonMisesTwoSamples(x,y)
p_value = 1/6*exp(-res)
y <- rgamma(num_of_samples, shape = 9, scale = 3)
res <- CramerVonMisesTwoSamples(x,y)
p_value = 1/6*exp(-res)
y <- rgamma(num_of_samples, shape = 11, scale = 3)
res <- CramerVonMisesTwoSamples(x,y)
p_value = 1/6*exp(-res)
y <- rgamma(num_of_samples, shape = 10, scale = 3)
result = ks.test(x, y)
num_of_samples = 1000
x <- rgamma(num_of_samples, shape = 10, scale = 3)
x <- x + rnorm(length(x), mean=0, sd = .1)
y <- rgamma(num_of_samples, shape = 10, scale = 3)
result = ks.test(x, y)
View(result)
View(result)
y <- rgamma(num_of_samples, shape = 9, scale = 3)
result = ks.test(x, y)
y <- rgamma(num_of_samples, shape = 11, scale = 3)
result = ks.test(x, y)
## Kolmogorov–Smirnov test
y <- rgamma(num_of_samples, shape = 10, scale = 3)
result = ks.test(x, y)
y <- rnorm(num_of_samples, 0, 1)
result = ks.test(x, y)
## Kolmogorov–Smirnov test
y <- rgamma(num_of_samples, shape = 10, scale = 3)
result = ks.test(x, y)
y <- rgamma(num_of_samples, shape = 11, scale = 3)
result = ks.test(x, y)
y <- rgamma(num_of_samples, shape = 9, scale = 3)
result = ks.test(x, y)
y <- rgamma(num_of_samples, shape = 10, scale = 3)
result = ks.test(x, y)
library('CDFt')
y <- rgamma(num_of_samples, shape = 10, scale = 3)
res <- CramerVonMisesTwoSamples(x,y)
p_value = 1/6*exp(-res)
y <- rnorm(num_of_samples,0,1)
res <- CramerVonMisesTwoSamples(x,y)
p_value = 1/6*exp(-res)
y <- rgamma(num_of_samples, shape = 10, scale = 3)
res <- CramerVonMisesTwoSamples(x,y)
p_value = 1/6*exp(-res)
library("TDA")# 四个子类
library('vsgoftest')
library(MASS)
source("EMC_mod.R")
# 每个类100个点
D = rnorm(n = 500, -5,1)
D = cbind(matrix(1, nr = 500, nc = 1), D)
D = rbind(D, cbind(matrix(2, nr = 300, nc = 1), rgamma(n = 300, 24, 4)))
D = rbind(D, cbind(matrix(3, nr = 400, nc = 1), rexp(n = 400, 2)))
X0 = D[,2:2]
X = scale(X0)
yy = density(X)
plot(yy, type="l", ylab="Density", xlab="x", las=1, lwd=2)
points(X, y=rep(0,nrow(X)), pch=1)
## (1) Nonparametric clustering
## (1.1) Enhanced mode clustering
X_emc = EMC(X)
cluster_lab = X_emc$labels
setwd("D:/科研/Code/nonparametric mixture/utils")
library("TDA")# 四个子类
library('vsgoftest')
library(MASS)
source("EMC_mod.R")
D = rnorm(n = 500, -5,1)
D = cbind(matrix(1, nr = 500, nc = 1), D)
D = rbind(D, cbind(matrix(2, nr = 300, nc = 1), rgamma(n = 300, 24, 4)))
D = rbind(D, cbind(matrix(3, nr = 400, nc = 1), rexp(n = 400, 2)))
X0 = D[,2:2]
X = scale(X0) ## scale only for EMC nonparametric clustering
yy = density(X0)
plot(yy, type="l", ylab="Density", xlab="x", las=1, lwd=2)
points(X0, y=rep(0,nrow(X)), pch=1)
## (1) Nonparametric clustering
## (1.1) Enhanced mode clustering
X_emc = EMC(X)
cluster_lab = X_emc$labels
## Visualization
plot(yy, type="l", ylab="Density", xlab="x", las=1, lwd=2)
points(X0, y=rep(0,nrow(X)), pch=1, col=rainbow(max(cluster_lab))[cluster_lab])
## (2) Goodness of fit (Google: Goodness of fit in R)
all_dist <- new.env()
all_dist[["1"]] <- "norm"
all_dist[["2"]] <- "lnorm"
all_dist[["3"]] <- "exp"
all_dist[["4"]] <- "gamma"
all_dist[["5"]] <- "f"
total_dist = 5
cluster_hash <- new.env() ## <cluster_lab, data points>
for(i in 1:max(cluster_lab)){
single_X = X0[which(cluster_lab==i)]
cluster_hash[[as.character(i)]] <- single_X
}
data = cluster_hash[[as.character(i)]]
nrow(data)
length(data)
data = cluster_hash[[as.character(1)]]
length(data)
data[data>0]
data
data[data>0]
data = cluster_hash[[as.character(2)]]
data[data>0]
length(data)
data = cluster_hash[[as.character(2)]]
data = cluster_hash[[as.character(1)]]
data = data[data>0]
data
length(data)
dist_hash <- new.env() ## <cluster_lab, distribution>
## distribution is a list, name = "distribution name", parameter = c(parametric vector)
for(i in 1:max(cluster_lab)){
data = cluster_hash[[as.character(i)]]
significant_level = 0.05
pvalue = 0
## fit the distribution
for (j in 1:total_dist) {
if (all_dist[[as.character(j)]] != "norm") {
## process data, leave only positive number
ori_length = length(data)
data = data[data>0]
if (length(data) < 0.5*ori_length) {
next
}
}
result = vs.test(x = data, densfun = paste("d",all_dist[[as.character(j)]],sep=""))
if (result$p.value > pvalue) {
pvalue = result
dist_hash[[as.character(i)]] <- list(name = all_dist[[as.character(j)]], parameter = result$estimate)
}
}
}
